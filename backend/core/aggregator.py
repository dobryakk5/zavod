"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
–°–æ–±–∏—Ä–∞–µ—Ç —Ç—Ä–µ–Ω–¥—ã –∏–∑ Google Trends –∏ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Google News RSS.
"""

import logging
from typing import List, Dict, Any
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


def fetch_google_trends(keywords: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ —Ç—Ä–µ–Ω–¥—ã –∏–∑ Google Trends –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.

    Args:
        keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–Ω–¥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç—Ä–µ–Ω–¥–æ–≤:
        {
            'title': str,
            'description': str,
            'url': str,
            'relevance_score': int,
            'extra': dict
        }
    """
    try:
        from pytrends.request import TrendReq

        pytrends = TrendReq(hl='ru-RU', tz=360)

        results = []

        # –ü–æ–ª—É—á–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        for keyword in keywords[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            try:
                # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è interest over time
                pytrends.build_payload([keyword], cat=0, timeframe='now 7-d', geo='', gprop='')

                # –ü–æ–ª—É—á–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                related_queries = pytrends.related_queries()

                if keyword in related_queries:
                    top_queries = related_queries[keyword].get('top')

                    if top_queries is not None and not top_queries.empty:
                        for idx, row in top_queries.head(limit).iterrows():
                            query = row['query']
                            value = int(row['value'])

                            results.append({
                                'title': f"{query} (—Å–≤—è–∑–∞–Ω–æ —Å '{keyword}')",
                                'description': f"–ü–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —Ç–µ–º–æ–π '{keyword}'",
                                'url': f"https://trends.google.com/trends/explore?q={query}",
                                'relevance_score': value,
                                'extra': {
                                    'keyword': keyword,
                                    'query': query,
                                    'value': value,
                                    'type': 'related_query'
                                }
                            })

                # –¢–∞–∫–∂–µ –ø–æ–ª—É—á–∏—Ç—å rising queries (—Ä–∞—Å—Ç—É—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã)
                rising_queries = related_queries[keyword].get('rising')

                if rising_queries is not None and not rising_queries.empty:
                    for idx, row in rising_queries.head(3).iterrows():
                        query = row['query']
                        value = row['value']

                        # Value –º–æ–∂–µ—Ç –±—ã—Ç—å "Breakout" –∏–ª–∏ —á–∏—Å–ª–æ–º
                        score = 1000 if value == "Breakout" else int(value) if isinstance(value, (int, float)) else 500

                        results.append({
                            'title': f"üî• {query} (—Ä–∞—Å—Ç—É—â–∏–π —Ç—Ä–µ–Ω–¥ –¥–ª—è '{keyword}')",
                            'description': f"–ë—ã—Å—Ç—Ä–æ —Ä–∞—Å—Ç—É—â–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ —Ç–µ–º–µ '{keyword}'",
                            'url': f"https://trends.google.com/trends/explore?q={query}",
                            'relevance_score': score,
                            'extra': {
                                'keyword': keyword,
                                'query': query,
                                'value': str(value),
                                'type': 'rising_query'
                            }
                        })

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è '{keyword}': {e}")
                continue

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ relevance_score –∏ –≤–µ—Ä–Ω—É—Ç—å —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:limit]

    except ImportError:
        logger.error("pytrends –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pytrends")
        return []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ Google Trends: {e}")
        return []


def fetch_google_news_rss(keywords: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Google News RSS –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.

    Args:
        keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π:
        {
            'title': str,
            'description': str,
            'url': str,
            'relevance_score': int,
            'extra': dict
        }
    """
    try:
        import feedparser
        import requests
        from urllib.parse import quote

        results = []

        for keyword in keywords[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            try:
                # Google News RSS URL
                # https://news.google.com/rss/search?q=–∫–ª—é—á–µ–≤–æ–µ+—Å–ª–æ–≤–æ&hl=ru&gl=RU&ceid=RU:ru
                encoded_keyword = quote(keyword)
                rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ru&gl=RU&ceid=RU:ru"

                logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è '{keyword}' –∏–∑ Google News RSS")

                # –ü–æ–ª—É—á–∏—Ç—å RSS feed
                feed = feedparser.parse(rss_url)

                # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø–∏—Å–∏
                for entry in feed.entries[:limit]:
                    title = entry.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                    link = entry.get('link', '')
                    description = entry.get('summary', '')
                    published = entry.get('published', '')
                    source = entry.get('source', {}).get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')

                    # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É
                    published_date = None
                    if published:
                        try:
                            from email.utils import parsedate_to_datetime
                            published_date = parsedate_to_datetime(published)
                        except:
                            pass

                    # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å relevance_score –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–µ–∂–µ—Å—Ç–∏ (–±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ = –≤—ã—à–µ score)
                    relevance_score = 50  # –±–∞–∑–æ–≤—ã–π score
                    if published_date:
                        age_hours = (datetime.now(published_date.tzinfo) - published_date).total_seconds() / 3600
                        # –ë–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–µ –±–∞–ª–ª–æ–≤
                        if age_hours < 24:
                            relevance_score = 100
                        elif age_hours < 48:
                            relevance_score = 80
                        elif age_hours < 72:
                            relevance_score = 60

                    results.append({
                        'title': title,
                        'description': description,
                        'url': link,
                        'relevance_score': relevance_score,
                        'extra': {
                            'keyword': keyword,
                            'source': source,
                            'published': published,
                            'published_date': published_date.isoformat() if published_date else None,
                        }
                    })

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è '{keyword}': {e}")
                continue

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ relevance_score
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:limit]

    except ImportError as e:
        logger.error(f"–ù–µ–æ–±—Ö–æ–¥–∏–º–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {e}. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install feedparser")
        return []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ Google News RSS: {e}")
        return []


def fetch_rss_feeds(feed_urls: List[str], keywords: List[str] = None, limit: int = 5) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS/Atom —Ñ–∏–¥–æ–≤.

    Args:
        feed_urls: –°–ø–∏—Å–æ–∫ URL RSS/Atom —Ñ–∏–¥–æ–≤
        keywords: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –≤—Å–µ)
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∫–∞–∂–¥–æ–≥–æ —Ñ–∏–¥–∞

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    try:
        import feedparser
        from datetime import datetime
        from email.utils import parsedate_to_datetime

        results = []

        for feed_url in feed_urls:
            try:
                logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ RSS —Ñ–∏–¥–∞: {feed_url}")
                feed = feedparser.parse(feed_url)

                for entry in feed.entries[:limit * 2]:  # –ë–µ—Ä—ë–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    title = entry.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                    link = entry.get('link', '')
                    description = entry.get('summary', entry.get('description', ''))
                    published = entry.get('published', entry.get('updated', ''))

                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
                    if keywords:
                        text_to_search = (title + ' ' + description).lower()
                        if not any(kw.lower() in text_to_search for kw in keywords):
                            continue

                    # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É
                    published_date = None
                    if published:
                        try:
                            published_date = parsedate_to_datetime(published)
                        except:
                            pass

                    # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å relevance_score –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–µ–∂–µ—Å—Ç–∏
                    relevance_score = 50
                    if published_date:
                        age_hours = (datetime.now(published_date.tzinfo) - published_date).total_seconds() / 3600
                        if age_hours < 24:
                            relevance_score = 100
                        elif age_hours < 48:
                            relevance_score = 80
                        elif age_hours < 72:
                            relevance_score = 60

                    results.append({
                        'title': title,
                        'description': description,
                        'url': link,
                        'relevance_score': relevance_score,
                        'extra': {
                            'feed_url': feed_url,
                            'published': published,
                            'published_date': published_date.isoformat() if published_date else None,
                        }
                    })

                    if len(results) >= limit * len(feed_urls):
                        break

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ RSS —Ñ–∏–¥–∞ '{feed_url}': {e}")
                continue

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ relevance_score
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:limit]

    except ImportError as e:
        logger.error(f"feedparser –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
        return []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ RSS —Ñ–∏–¥–æ–≤: {e}")
        return []


def fetch_youtube_videos(api_key: str, channel_ids: List[str], keywords: List[str] = None, limit: int = 5) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∏–¥–µ–æ –∏–∑ YouTube –∫–∞–Ω–∞–ª–æ–≤.

    Args:
        api_key: YouTube Data API v3 –∫–ª—é—á
        channel_ids: –°–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤ –∏–ª–∏ handles (–Ω–∞–ø—Ä–∏–º–µ—Ä, UC_x5XG1OV2P6uZZ5FSM9Ttw –∏–ª–∏ @channel_handle)
        keywords: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ —Å –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –≤–∏–¥–µ–æ
    """
    try:
        import requests
        from datetime import datetime
        from urllib.parse import quote

        if not api_key:
            logger.error("YouTube API –∫–ª—é—á –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            return []

        results = []

        for channel_id in channel_ids:
            try:
                # –ï—Å–ª–∏ channel_id –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å @, –Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π ID
                if channel_id.startswith('@'):
                    # –ü–æ–∏—Å–∫ –∫–∞–Ω–∞–ª–∞ –ø–æ handle
                    search_url = "https://www.googleapis.com/youtube/v3/search"
                    search_params = {
                        'key': api_key,
                        'q': channel_id,
                        'type': 'channel',
                        'part': 'snippet',
                        'maxResults': 1
                    }
                    search_response = requests.get(search_url, params=search_params)
                    search_response.raise_for_status()
                    search_data = search_response.json()

                    if not search_data.get('items'):
                        logger.warning(f"–ö–∞–Ω–∞–ª {channel_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                        continue

                    channel_id = search_data['items'][0]['snippet']['channelId']

                # –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∏–¥–µ–æ –∫–∞–Ω–∞–ª–∞
                logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ –∏–∑ YouTube –∫–∞–Ω–∞–ª–∞: {channel_id}")

                search_url = "https://www.googleapis.com/youtube/v3/search"
                params = {
                    'key': api_key,
                    'channelId': channel_id,
                    'part': 'snippet',
                    'order': 'date',
                    'type': 'video',
                    'maxResults': limit * 2  # –ë–µ—Ä—ë–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                }

                response = requests.get(search_url, params=params)
                response.raise_for_status()
                data = response.json()

                for item in data.get('items', []):
                    video_id = item['id']['videoId']
                    snippet = item['snippet']

                    title = snippet.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                    description = snippet.get('description', '')
                    published = snippet.get('publishedAt', '')
                    thumbnail = snippet.get('thumbnails', {}).get('high', {}).get('url', '')

                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                    if keywords:
                        text_to_search = (title + ' ' + description).lower()
                        if not any(kw.lower() in text_to_search for kw in keywords):
                            continue

                    # –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–∏–¥–µ–æ (–ø—Ä–æ—Å–º–æ—Ç—Ä—ã, –ª–∞–π–∫–∏)
                    stats_url = "https://www.googleapis.com/youtube/v3/videos"
                    stats_params = {
                        'key': api_key,
                        'id': video_id,
                        'part': 'statistics'
                    }
                    stats_response = requests.get(stats_url, params=stats_params)
                    stats_data = stats_response.json()

                    view_count = 0
                    like_count = 0
                    if stats_data.get('items'):
                        statistics = stats_data['items'][0].get('statistics', {})
                        view_count = int(statistics.get('viewCount', 0))
                        like_count = int(statistics.get('likeCount', 0))

                    # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å relevance_score –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏ —Å–≤–µ–∂–µ—Å—Ç–∏
                    relevance_score = min(view_count // 100, 1000)  # –ú–∞–∫—Å 1000

                    # –î–æ–±–∞–≤–∏—Ç—å –±–æ–Ω—É—Å –∑–∞ —Å–≤–µ–∂–µ—Å—Ç—å
                    if published:
                        try:
                            published_date = datetime.fromisoformat(published.replace('Z', '+00:00'))
                            age_hours = (datetime.now(published_date.tzinfo) - published_date).total_seconds() / 3600
                            if age_hours < 24:
                                relevance_score += 200
                            elif age_hours < 48:
                                relevance_score += 100
                        except:
                            pass

                    results.append({
                        'title': title,
                        'description': description,
                        'url': f"https://www.youtube.com/watch?v={video_id}",
                        'relevance_score': relevance_score,
                        'extra': {
                            'video_id': video_id,
                            'channel_id': channel_id,
                            'published': published,
                            'thumbnail': thumbnail,
                            'view_count': view_count,
                            'like_count': like_count,
                        }
                    })

                    if len(results) >= limit * len(channel_ids):
                        break

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∫–∞–Ω–∞–ª–∞ '{channel_id}': {e}")
                continue

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ relevance_score
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:limit]

    except ImportError as e:
        logger.error(f"requests –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
        return []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ YouTube –≤–∏–¥–µ–æ: {e}")
        return []


def fetch_instagram_posts(access_token: str, usernames: List[str], keywords: List[str] = None, limit: int = 5) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ Instagram –∞–∫–∫–∞—É–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ Graph API.

    –í–ù–ò–ú–ê–ù–ò–ï: –î–ª—è —Ä–∞–±–æ—Ç—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è Instagram Business –∏–ª–∏ Creator –∞–∫–∫–∞—É–Ω—Ç
    –∏ access_token —Å –ø—Ä–∞–≤–∞–º–∏ instagram_basic, instagram_content_publish.

    Args:
        access_token: Instagram Graph API —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞
        usernames: –°–ø–∏—Å–æ–∫ Instagram –∞–∫–∫–∞—É–Ω—Ç–æ–≤ (usernames)
        keywords: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ —Å –∫–∞–∂–¥–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å—Ç–æ–≤
    """
    try:
        import requests

        if not access_token:
            logger.error("Instagram access token –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            return []

        results = []

        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: Instagram Graph API —Ç—Ä–µ–±—É–µ—Ç –±–∏–∑–Ω–µ—Å-–∞–∫–∫–∞—É–Ω—Ç—ã
        # –≠—Ç–æ—Ç –∫–æ–¥ —è–≤–ª—è–µ—Ç—Å—è –∑–∞–≥–ª—É—à–∫–æ–π –∏ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        logger.warning("Instagram API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∏–∑–Ω–µ—Å-–∞–∫–∫–∞—É–Ω—Ç–∞ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π")
        logger.warning("–§—É–Ω–∫—Ü–∏—è fetch_instagram_posts —è–≤–ª—è–µ—Ç—Å—è –∑–∞–≥–ª—É—à–∫–æ–π –∏ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")

        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Instagram Graph API
        # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://developers.facebook.com/docs/instagram-api/

        return results

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ Instagram –ø–æ—Å—Ç–æ–≤: {e}")
        return []


def fetch_vkontakte_posts(access_token: str, group_ids: List[str], keywords: List[str] = None, limit: int = 5) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ VK –≥—Ä—É–ø–ø —á–µ—Ä–µ–∑ VK API.

    Args:
        access_token: VK API —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞
        group_ids: –°–ø–∏—Å–æ–∫ ID –≥—Ä—É–ø–ø –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–º–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä: apiclub, thecode)
        keywords: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ —Å –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å—Ç–æ–≤
    """
    try:
        import requests
        from datetime import datetime

        if not access_token:
            logger.error("VKontakte access token –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            return []

        results = []
        api_version = "5.131"

        for group_id in group_ids:
            try:
                # –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã —Å–æ —Å—Ç–µ–Ω—ã –≥—Ä—É–ø–ø—ã
                logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –∏–∑ VK –≥—Ä—É–ø–ø—ã: {group_id}")

                url = "https://api.vk.com/method/wall.get"
                params = {
                    'access_token': access_token,
                    'v': api_version,
                    'domain': group_id,  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–º—è –∏–ª–∏ ID
                    'count': limit * 2,  # –ë–µ—Ä—ë–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    'filter': 'owner'  # –¢–æ–ª—å–∫–æ –ø–æ—Å—Ç—ã –≤–ª–∞–¥–µ–ª—å—Ü–∞ (–Ω–µ —Ä–µ–ø–æ—Å—Ç—ã)
                }

                response = requests.get(url, params=params)
                response.raise_for_status()
                data = response.json()

                if 'error' in data:
                    logger.error(f"VK API –æ—à–∏–±–∫–∞ –¥–ª—è –≥—Ä—É–ø–ø—ã {group_id}: {data['error'].get('error_msg')}")
                    continue

                items = data.get('response', {}).get('items', [])

                for item in items:
                    post_id = item.get('id')
                    owner_id = item.get('owner_id')
                    text = item.get('text', '')
                    date = item.get('date', 0)
                    likes = item.get('likes', {}).get('count', 0)
                    reposts = item.get('reposts', {}).get('count', 0)
                    views = item.get('views', {}).get('count', 0)
                    comments = item.get('comments', {}).get('count', 0)

                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                    if keywords:
                        if not any(kw.lower() in text.lower() for kw in keywords):
                            continue

                    # –ü–æ–ª—É—á–∏—Ç—å URL –ø–æ—Å—Ç–∞
                    post_url = f"https://vk.com/wall{owner_id}_{post_id}"

                    # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å relevance_score –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
                    relevance_score = likes + reposts * 3 + comments * 2 + (views // 100 if views else 0)

                    # –î–æ–±–∞–≤–∏—Ç—å –±–æ–Ω—É—Å –∑–∞ —Å–≤–µ–∂–µ—Å—Ç—å
                    if date:
                        try:
                            post_date = datetime.fromtimestamp(date)
                            now = datetime.now()
                            age_hours = (now - post_date).total_seconds() / 3600
                            if age_hours < 24:
                                relevance_score += 100
                            elif age_hours < 48:
                                relevance_score += 50
                        except:
                            pass

                    # –û–±—Ä–µ–∑–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è title
                    title = text[:200] if len(text) > 200 else text
                    if not title:
                        title = f"–ü–æ—Å—Ç –æ—Ç {group_id}"

                    results.append({
                        'title': title,
                        'description': text,
                        'url': post_url,
                        'relevance_score': relevance_score,
                        'extra': {
                            'group_id': group_id,
                            'post_id': post_id,
                            'owner_id': owner_id,
                            'date': datetime.fromtimestamp(date).isoformat() if date else None,
                            'likes': likes,
                            'reposts': reposts,
                            'views': views,
                            'comments': comments,
                        }
                    })

                    if len(results) >= limit * len(group_ids):
                        break

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ—Å—Ç–æ–≤ –∏–∑ VK –≥—Ä—É–ø–ø—ã '{group_id}': {e}")
                continue

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ relevance_score
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:limit]

    except ImportError as e:
        logger.error(f"requests –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
        return []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ VK –ø–æ—Å—Ç–æ–≤: {e}")
        return []


def deduplicate_trends(trends: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ URL.

    Args:
        trends: –°–ø–∏—Å–æ–∫ —Ç—Ä–µ–Ω–¥–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤
    """
    seen_urls = set()
    unique_trends = []

    for trend in trends:
        url = trend.get('url', '')
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique_trends.append(trend)
        elif not url:
            # –ï—Å–ª–∏ –Ω–µ—Ç URL, –≤—Å—ë —Ä–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º)
            unique_trends.append(trend)

    return unique_trends
